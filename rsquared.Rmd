---
title: "rsquared"
description: "A new article created using the Distill format"
date: "12/8/2021"
output: distill::distill_article
---

Rsquared is interesting in the fact that it does nothing about prediction error. Each time you can get a different number anywhere between 0 and 1. We demonstrated this in class when one student asked why her numbers didn't match yours. And mine didn't match either one either. Thats the most interesting thing about Rsquared. In most cases, it will not give the same number twice which can be a problem with Rsquared. 


[R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination) is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.

In R, we typically get R-squared by calling the summary function on a model object. Here's a quick example using simulated data:

```{r,echo=TRUE}
# independent variable
x <- 1:20 
# for reproducibility
set.seed(1) 
# dependent variable; function of x with random error
y <- 2 + 0.5*x + rnorm(20,0,3) 
# simple linear regression
mod <- lm(y~x)
# request just the r-squared value
summary(mod)$r.squared          
```

One way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:

$$
R^{2} =  \frac{\sum (\hat{y} – \bar{\hat{y}})^{2}}{\sum (y – \bar{y})^{2}}
$$

We can calculate it directly using our model object like so:

```{r, echo=TRUE}
# extract fitted (or predicted) values from model
f <- mod$fitted.values
# sum of squared fitted-value deviations
mss <- sum((f - mean(f))^2)
# sum of squared original-value deviations
tss <- sum((y - mean(y))^2)
# r-squared
mss/tss                      
```


3\. *R-squared says nothing about prediction error, even with* $σ^2$ exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We're better off using Mean Square Error (MSE) as a measure of prediction error.

MSE is basically the fitted y values minus the observed y values, squared, then summed, and then divided by the number of observations.

Let's demonstrate this statement by first generating data that meets all simple linear regression assumptions and then regressing y on x to assess both R-squared and MSE.

```{r,echo=TRUE}
x <- seq(1,10,length.out = 100)
set.seed(1)
y <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)
mod1 <- lm(y ~ x)
summary(mod1)$r.squared
# Mean squared error
sum((fitted(mod1) - y)^2)/100
```

Now repeat the above code, but this time with a different range of x. Leave everything else the same:

```{r,echo=TRUE}
 # new range of x
x <- seq(1,2,length.out = 100)      
set.seed(1)
y <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)
mod1 <- lm(y ~ x)
summary(mod1)$r.squared
# Mean squared error
sum((fitted(mod1) - y)^2)/100        
```

The R-squared falls from 0.94 to 0.15 but the MSE remains the same. In other words the predictive ability is the same for both data sets, but the R-squared would lead you to believe the first example somehow had a model with more predictive power.


Let's recap:

-   R-squared does not measure goodness of fit.

-   R-squared does not measure predictive error.

-   R-squared does not necessarily increase when assumptions are better satisfied.

-   R-squared does not measure how one variable explains another.

To better measure the predictive error Mean Square Error (MSE) as a measure is better suited. As was shown, it did not change and thus creating a more reliable predictive measure. 
